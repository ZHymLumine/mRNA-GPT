import os
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import r2_score
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
import joblib

# === æ­¥éª¤1: æ•°æ®è¯»å–ä¸å¤„ç† ===
print("æ­£åœ¨è¯»å–æ•°æ®...")
df = pd.read_csv("ecoli_TE_CDS_final.csv")
cds_list = df["CDS_sequence"].astype(str).str.upper().str.replace("U", "T").str.replace("N", "A")
mrl_list = df["TE"].values.astype(float)

def extract_codons(seq):
    return [seq[i:i+3] for i in range(0, len(seq), 3) if len(seq[i:i+3]) == 3 and set(seq[i:i+3]).issubset("ATCG")]

# æå–å¯†ç å­å¹¶è¿‡æ»¤æ— æ•ˆåºåˆ—
codon_seqs = [extract_codons(seq) for seq in cds_list]
filtered = [(seq, mrl) for seq, mrl in zip(codon_seqs, mrl_list) if len(seq) > 0]
codon_seqs = [f[0] for f in filtered]
y = [f[1] for f in filtered]

# è®¡ç®—æ¯ä¸ªåºåˆ—çš„å¯†ç å­é¢‘ç‡ç‰¹å¾
all_codons = sorted(set(c for seq in codon_seqs for c in seq))
print(f"æ€»å…±æœ‰ {len(all_codons)} ç§å¯†ç å­")

# ä¸ºæ¯ä¸ªåºåˆ—åˆ›å»ºå¯†ç å­é¢‘ç‡ç‰¹å¾
X_features = []
for seq in codon_seqs:
    codon_count = {codon: 0 for codon in all_codons}
    for codon in seq:
        codon_count[codon] += 1
    
    # è®¡ç®—é¢‘ç‡è€Œä¸æ˜¯è®¡æ•°
    total_codons = len(seq)
    codon_freq = {codon: count/total_codons for codon, count in codon_count.items()}
    
    # æ·»åŠ å…¶ä»–å¯èƒ½æœ‰ç”¨çš„ç‰¹å¾
    features = list(codon_freq.values())
    features.append(len(seq))  # æ·»åŠ åºåˆ—é•¿åº¦ä½œä¸ºç‰¹å¾
    
    X_features.append(features)

X = np.array(X_features)
print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")

# æ ‡å‡†åŒ–ç‰¹å¾
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
y_array = np.array(y)

# === æ­¥éª¤2: æ•°æ®åˆ’åˆ† ===
print("åˆ’åˆ†æ•°æ®é›†...")
X_temp, X_test, y_temp, y_test = train_test_split(X_scaled, y_array, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)

# === æ­¥éª¤3: éšæœºæ£®æ—æ¨¡å‹è®­ç»ƒ ===
print("è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
model = RandomForestRegressor(
    n_estimators=200,
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='sqrt',
    n_jobs=-1,
    random_state=42
)

model.fit(X_train, y_train)

# === æ­¥éª¤4: æ¨¡å‹è¯„ä¼° ===
print("è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
# éªŒè¯é›†è¯„ä¼°
y_val_pred = model.predict(X_val)
val_r2 = r2_score(y_val, y_val_pred)
val_pearson, val_p = pearsonr(y_val, y_val_pred)
print(f"éªŒè¯é›†: R^2 = {val_r2:.4f}, Pearson r = {val_pearson:.4f}, p = {val_p:.2g}")

# æµ‹è¯•é›†è¯„ä¼°
y_test_pred = model.predict(X_test)
test_r2 = r2_score(y_test, y_test_pred)
test_pearson, test_p = pearsonr(y_test, y_test_pred)
print(f"æµ‹è¯•é›†: R^2 = {test_r2:.4f}, Pearson r = {test_pearson:.4f}, p = {test_p:.2g}")

# === æ­¥éª¤5: ç‰¹å¾é‡è¦æ€§åˆ†æ ===
feature_names = all_codons + ["åºåˆ—é•¿åº¦"]
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("éšæœºæ£®æ—æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(min(20, X.shape[1])), [feature_names[i] for i in indices[:20]], rotation=90)
plt.tight_layout()
plt.savefig("rf_feature_importance.png")

# === æ­¥éª¤6: ä¿å­˜é¢„æµ‹ç»“æœå’Œæ¨¡å‹ ===
# ä¿å­˜é¢„æµ‹æ•£ç‚¹å›¾
plt.figure(figsize=(6, 5))
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.xlabel("å®é™…ç¿»è¯‘æ•ˆç‡")
plt.ylabel("é¢„æµ‹ç¿»è¯‘æ•ˆç‡")
plt.title("å®é™…å€¼ vs é¢„æµ‹å€¼ (éšæœºæ£®æ—)")
plt.grid(True)
plt.text(0.05, 0.95,
         f"Pearson r = {test_pearson:.4f}\np = {test_p:.2g}",
         transform=plt.gca().transAxes,
         verticalalignment='top',
         fontsize=12,
         bbox=dict(boxstyle="round", fc="w", ec="gray", alpha=0.6))
plt.tight_layout()
plt.savefig("pearson_scatter_randomforest.png")

# ä¿å­˜é¢„æµ‹ç»“æœ
df_result = pd.DataFrame({
    "å®é™…å€¼": y_test,
    "é¢„æµ‹å€¼": y_test_pred
})
df_result.to_csv("pred_vs_actual_randomforest.csv", index=False)

# ä¿å­˜æ¨¡å‹
joblib.dump(model, "best_randomforest_model.joblib")
joblib.dump(scaler, "feature_scaler.joblib")
joblib.dump(feature_names, "feature_names.joblib")

print("ğŸ“¦ ä¿å­˜å®Œæ¯•ï¼šbest_randomforest_model.joblibã€feature_scaler.joblibã€feature_names.joblibã€rf_feature_importance.pngã€pearson_scatter_randomforest.pngã€pred_vs_actual_randomforest.csv") 